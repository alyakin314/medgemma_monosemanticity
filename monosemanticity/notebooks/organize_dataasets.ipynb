{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff7715c-b8f0-48c5-9c59-5577dd4ed296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 56658\n",
      "max turns in a conversation: 10\n",
      "samples with NO image:       0\n",
      "image NOT in first turn:     0\n",
      "samples with multiple images: 0\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import json\n",
    "\n",
    "# point this at your actual data file\n",
    "data_path = \"/gpfs/data/oermannlab/public_data/llava-med-data/llava_med_instruct_60k_inline_mention_filtered.json\"\n",
    "\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"total samples: {len(data)}\")\n",
    "\n",
    "# scan all conversations\n",
    "no_image = 0\n",
    "image_not_first_turn = 0\n",
    "multi_image = 0\n",
    "max_turns = 0\n",
    "\n",
    "for entry in data:\n",
    "    convos = entry[\"conversations\"]\n",
    "    max_turns = max(max_turns, len(convos))\n",
    "    \n",
    "    # find all image blocks\n",
    "    image_positions = []  # (turn_index, block_index)\n",
    "    for t_idx, turn in enumerate(convos):\n",
    "        content = turn.get(\"content\", [])\n",
    "        if isinstance(content, str):\n",
    "            # old format?\n",
    "            if \"<image>\" in content:\n",
    "                image_positions.append((t_idx, -1))\n",
    "            continue\n",
    "        for b_idx, block in enumerate(content):\n",
    "            if isinstance(block, dict) and block.get(\"type\") == \"image\":\n",
    "                image_positions.append((t_idx, b_idx))\n",
    "    \n",
    "    if len(image_positions) == 0:\n",
    "        no_image += 1\n",
    "    elif image_positions[0][0] != 0:\n",
    "        image_not_first_turn += 1\n",
    "    if len(image_positions) > 1:\n",
    "        multi_image += 1\n",
    "\n",
    "print(f\"max turns in a conversation: {max_turns}\")\n",
    "print(f\"samples with NO image:       {no_image}\")\n",
    "print(f\"image NOT in first turn:     {image_not_first_turn}\")\n",
    "print(f\"samples with multiple images: {multi_image}\")\n",
    "\n",
    "# show a couple of edge cases if they exist\n",
    "if no_image > 0:\n",
    "    for entry in data:\n",
    "        has_img = any(\n",
    "            isinstance(b, dict) and b.get(\"type\") == \"image\"\n",
    "            for t in entry[\"conversations\"]\n",
    "            for b in (t.get(\"content\", []) if isinstance(t.get(\"content\"), list) else [])\n",
    "        )\n",
    "        if not has_img:\n",
    "            print(f\"\\nExample no-image sample:\")\n",
    "            print(json.dumps(entry[\"conversations\"][:2], indent=2))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46b7b71-4b00-4f27-8d08-7a8fa204642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data/oermannlab/users/alyaka01/.conda/envs/monosemanticity/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The image processor of type `LlavaNextImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TOKENIZER COMPARISON: LLaVA-Next-34B vs Gemma 3\n",
      "======================================================================\n",
      "  vocab_size                 llava=64000                 gemma=262144                <-- DIFFERS\n",
      "  model_max_length           llava=4096                  gemma=1000000000000000019884624838656  <-- DIFFERS\n",
      "  padding_side               llava=right                 gemma=left                  <-- DIFFERS\n",
      "  truncation_side            llava=right                 gemma=right               \n",
      "  pad_token                  llava=<unk>                 gemma=<pad>                 <-- DIFFERS\n",
      "  pad_token_id               llava=0                     gemma=0                   \n",
      "  eos_token                  llava=<|im_end|>            gemma=<eos>                 <-- DIFFERS\n",
      "  eos_token_id               llava=7                     gemma=1                     <-- DIFFERS\n",
      "  bos_token                  llava=<|startoftext|>       gemma=<bos>                 <-- DIFFERS\n",
      "  bos_token_id               llava=1                     gemma=2                     <-- DIFFERS\n",
      "  unk_token                  llava=<unk>                 gemma=<unk>               \n",
      "  unk_token_id               llava=0                     gemma=3                     <-- DIFFERS\n",
      "\n",
      "  all_special_tokens:\n",
      "    shared:         {'<unk>'}\n",
      "    only in llava:  {'<|im_end|>', '<|startoftext|>', '<image>'}\n",
      "    only in gemma:  {'<start_of_image>', '<mask>', '<end_of_image>', '<pad>', '<image_soft_token>', '<bos>', '<eos>'}\n",
      "\n",
      "  IFT-relevant tokens (old vs new):\n",
      "    LLaVA-Next-34B:\n",
      "      <|im_start|>         -> 6\n",
      "      <|im_end|>           -> 7\n",
      "      assistant            -> [14135]\n",
      "      user                 -> [2942]\n",
      "    Gemma 3:\n",
      "      <start_of_turn>      -> 105\n",
      "      <end_of_turn>        -> 106\n",
      "      model                -> [4368]\n",
      "      user                 -> [2364]\n",
      "\n",
      "======================================================================\n",
      "PROCESSOR COMPARISON\n",
      "======================================================================\n",
      "  image_token                llava=<image>               gemma=<image_soft_token>    <-- DIFFERS\n",
      "  image_token_id             llava=64000                 gemma=262144                <-- DIFFERS\n",
      "\n",
      "======================================================================\n",
      "IMAGE PROCESSOR COMPARISON\n",
      "======================================================================\n",
      "  type                                 llava=LlavaNextImageProcessorFast     gemma=Gemma3ImageProcessorFast\n",
      "  do_resize                            llava=True                            gemma=True                          \n",
      "  do_rescale                           llava=True                            gemma=True                          \n",
      "  do_normalize                         llava=True                            gemma=True                          \n",
      "  do_convert_rgb                       llava=True                            gemma=None                            <-- DIFFERS\n",
      "  image_mean                           llava=[0.48145466, 0.4578275, 0.40821073]  gemma=[0.5, 0.5, 0.5]                 <-- DIFFERS\n",
      "  image_std                            llava=[0.26862954, 0.26130258, 0.27577711]  gemma=[0.5, 0.5, 0.5]                 <-- DIFFERS\n",
      "  size                                 llava={'shortest_edge': 336}          gemma={'height': 896, 'width': 896}   <-- DIFFERS\n",
      "  resample                             llava=3                               gemma=2                               <-- DIFFERS\n",
      "  rescale_factor                       llava=0.00392156862745098             gemma=0.00392156862745098           \n",
      "  do_pan_and_scan                      llava=N/A                             gemma=None                            (gemma only)\n",
      "  image_seq_length                     llava=N/A                             gemma=256                             (gemma only)\n",
      "  pan_and_scan_max_num_crops           llava=N/A                             gemma=None                            (gemma only)\n",
      "  pan_and_scan_min_crop_size           llava=N/A                             gemma=None                            (gemma only)\n",
      "\n",
      "======================================================================\n",
      "PIXEL VALUES (actual image: (2412, 1956))\n",
      "======================================================================\n",
      "  llava pixel_values: shape=torch.Size([1, 5, 3, 336, 336])  dtype=torch.float32  min=-1.792  max=1.591  mean=-0.272\n",
      "  gemma pixel_values: shape=torch.Size([1, 3, 896, 896])  dtype=torch.float32  min=-1.000  max=0.694  mean=-0.113\n",
      "\n",
      "======================================================================\n",
      "CHAT TEMPLATE OUTPUT (same conversation, same image)\n",
      "======================================================================\n",
      "  Gemma 3:\n",
      "<bos><start_of_turn>user\n",
      "<start_of_image>What is this scan?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "This is a chest X-ray showing no acute findings.<end_of_turn>\n",
      "\n",
      "\n",
      "  LLaVA-Next-34B (HF format):\n",
      "<|im_start|>user\n",
      "<image>\n",
      "What is this scan?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "This is a chest X-ray showing no acute findings.<|im_end|>\n",
      "\n",
      "\n",
      "======================================================================\n",
      "FULL PROCESSOR CALL (text + image)\n",
      "======================================================================\n",
      "  Gemma 3 batch keys:  ['attention_mask', 'input_ids', 'pixel_values', 'token_type_ids']\n",
      "    input_ids:    torch.Size([1, 288])\n",
      "    pixel_values: torch.Size([1, 3, 896, 896])\n",
      "  LLaVA full processor call failed: unsupported operand type(s) for //: 'int' and 'NoneType'\n",
      "\n",
      "======================================================================\n",
      "UNTOKENIZED PROMPTS (decoded from input_ids)\n",
      "======================================================================\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaNextProcessor\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "# grab a test image\n",
    "url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "r = requests.get(url, headers=headers, timeout=30)\n",
    "r.raise_for_status()\n",
    "\n",
    "# optional but very helpful sanity check\n",
    "ct = r.headers.get(\"Content-Type\", \"\")\n",
    "if \"image\" not in ct:\n",
    "    raise ValueError(f\"Expected image, got Content-Type={ct}. First bytes: {r.content[:120]!r}\")\n",
    "\n",
    "image = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "\n",
    "llava_proc = LlavaNextProcessor.from_pretrained(\"NYU-OLAB/LLaVA-Next-Med-OLAB\")\n",
    "gemma_proc = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "l_tok = llava_proc.tokenizer\n",
    "g_tok = gemma_proc.tokenizer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOKENIZER COMPARISON: LLaVA-Next-34B vs Gemma 3\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "attrs = [\n",
    "    \"vocab_size\", \"model_max_length\", \"padding_side\", \"truncation_side\",\n",
    "    \"pad_token\", \"pad_token_id\", \"eos_token\", \"eos_token_id\",\n",
    "    \"bos_token\", \"bos_token_id\", \"unk_token\", \"unk_token_id\",\n",
    "]\n",
    "for attr in attrs:\n",
    "    lv = getattr(l_tok, attr, \"N/A\")\n",
    "    gv = getattr(g_tok, attr, \"N/A\")\n",
    "    flag = \"\" if lv == gv else \"  <-- DIFFERS\"\n",
    "    print(f\"  {attr:25s}  llava={str(lv):20s}  gemma={str(gv):20s}{flag}\")\n",
    "\n",
    "# special tokens\n",
    "print(f\"\\n  all_special_tokens:\")\n",
    "l_special = set(l_tok.all_special_tokens)\n",
    "g_special = set(g_tok.all_special_tokens)\n",
    "shared = l_special & g_special\n",
    "only_llava = l_special - g_special\n",
    "only_gemma = g_special - l_special\n",
    "print(f\"    shared:         {shared}\")\n",
    "if only_llava:\n",
    "    print(f\"    only in llava:  {only_llava}\")\n",
    "if only_gemma:\n",
    "    print(f\"    only in gemma:  {only_gemma}\")\n",
    "\n",
    "# the tokens that matter for IFT masking\n",
    "print(f\"\\n  IFT-relevant tokens (old vs new):\")\n",
    "# the 34B uses ChatML with <|im_start|> / <|im_end|>\n",
    "llava_ift_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\n",
    "print(f\"    LLaVA-Next-34B:\")\n",
    "for tok in llava_ift_tokens:\n",
    "    tid = l_tok.convert_tokens_to_ids(tok)\n",
    "    print(f\"      {tok:20s} -> {tid}\")\n",
    "# also check the role words\n",
    "for role_word in [\"assistant\", \"user\"]:\n",
    "    tid = l_tok.encode(role_word, add_special_tokens=False)\n",
    "    print(f\"      {role_word:20s} -> {tid}\")\n",
    "\n",
    "print(f\"    Gemma 3:\")\n",
    "gemma_ift_tokens = [\"<start_of_turn>\", \"<end_of_turn>\"]\n",
    "for tok in gemma_ift_tokens:\n",
    "    tid = g_tok.convert_tokens_to_ids(tok)\n",
    "    print(f\"      {tok:20s} -> {tid}\")\n",
    "for role_word in [\"model\", \"user\"]:\n",
    "    tid = g_tok.encode(role_word, add_special_tokens=False)\n",
    "    print(f\"      {role_word:20s} -> {tid}\")\n",
    "\n",
    "# --- Processor-level ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROCESSOR COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for attr in [\"image_token\", \"image_token_id\"]:\n",
    "    lv = getattr(llava_proc, attr, \"N/A\")\n",
    "    gv = getattr(gemma_proc, attr, \"N/A\")\n",
    "    flag = \"\" if lv == gv else \"  <-- DIFFERS\"\n",
    "    print(f\"  {attr:25s}  llava={str(lv):20s}  gemma={str(gv):20s}{flag}\")\n",
    "\n",
    "# --- Image processor ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMAGE PROCESSOR COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "l_ip = llava_proc.image_processor\n",
    "g_ip = gemma_proc.image_processor\n",
    "\n",
    "print(f\"  {'type':35s}  llava={type(l_ip).__name__:30s}  gemma={type(g_ip).__name__}\")\n",
    "ip_attrs = [\n",
    "    \"do_resize\", \"do_rescale\", \"do_normalize\", \"do_convert_rgb\",\n",
    "    \"image_mean\", \"image_std\", \"size\", \"resample\",\n",
    "    \"rescale_factor\",\n",
    "]\n",
    "for attr in ip_attrs:\n",
    "    lv = getattr(l_ip, attr, \"N/A\")\n",
    "    gv = getattr(g_ip, attr, \"N/A\")\n",
    "    flag = \"\" if lv == gv else \"  <-- DIFFERS\"\n",
    "    print(f\"  {attr:35s}  llava={str(lv):30s}  gemma={str(gv):30s}{flag}\")\n",
    "\n",
    "# gemma-specific attrs\n",
    "for attr in [\"do_pan_and_scan\", \"image_seq_length\",\n",
    "             \"pan_and_scan_max_num_crops\", \"pan_and_scan_min_crop_size\"]:\n",
    "    gv = getattr(g_ip, attr, \"N/A\")\n",
    "    print(f\"  {attr:35s}  llava={'N/A':30s}  gemma={str(gv):30s}  (gemma only)\")\n",
    "\n",
    "# --- Process the actual image through both ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"PIXEL VALUES (actual image: {image.size})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "l_pix = llava_proc.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "g_pix = gemma_proc.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "print(f\"  llava pixel_values: shape={l_pix.shape}  dtype={l_pix.dtype}  \"\n",
    "      f\"min={l_pix.min():.3f}  max={l_pix.max():.3f}  mean={l_pix.mean():.3f}\")\n",
    "print(f\"  gemma pixel_values: shape={g_pix.shape}  dtype={g_pix.dtype}  \"\n",
    "      f\"min={g_pix.min():.3f}  max={g_pix.max():.3f}  mean={g_pix.mean():.3f}\")\n",
    "\n",
    "# --- Chat template output ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHAT TEMPLATE OUTPUT (same conversation, same image)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "convo_hf = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": \"What is this scan?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"This is a chest X-ray showing no acute findings.\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "g_text = gemma_proc.apply_chat_template(convo_hf, tokenize=False, add_generation_prompt=False)\n",
    "print(f\"  Gemma 3:\\n{g_text}\\n\")\n",
    "\n",
    "# LLaVA-Next 34B uses ChatML — try HF format first, fall back to old format\n",
    "try:\n",
    "    l_text = llava_proc.apply_chat_template(convo_hf, tokenize=False, add_generation_prompt=False)\n",
    "    print(f\"  LLaVA-Next-34B (HF format):\\n{l_text}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"  LLaVA-Next-34B HF format failed: {e}\")\n",
    "    llava_convo = [\n",
    "        {\"role\": \"user\", \"content\": \"<image>\\nWhat is this scan?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"This is a chest X-ray showing no acute findings.\"},\n",
    "    ]\n",
    "    l_text = llava_proc.apply_chat_template(llava_convo)\n",
    "    print(f\"  LLaVA-Next-34B (old format):\\n{l_text}\\n\")\n",
    "\n",
    "# --- Full processor call with image ---\n",
    "print(\"=\" * 70)\n",
    "print(\"FULL PROCESSOR CALL (text + image)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "g_batch = gemma_proc(text=[g_text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "print(f\"  Gemma 3 batch keys:  {sorted(g_batch.keys())}\")\n",
    "print(f\"    input_ids:    {g_batch['input_ids'].shape}\")\n",
    "print(f\"    pixel_values: {g_batch['pixel_values'].shape}\")\n",
    "\n",
    "try:\n",
    "    l_batch = llava_proc(text=[l_text], images=[image], padding=True, return_tensors=\"pt\")\n",
    "    print(f\"  LLaVA batch keys:    {sorted(l_batch.keys())}\")\n",
    "    print(f\"    input_ids:    {l_batch['input_ids'].shape}\")\n",
    "    print(f\"    pixel_values: {l_batch['pixel_values'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"  LLaVA full processor call failed: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UNTOKENIZED PROMPTS (decoded from input_ids)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ecc450-908b-40f2-ac6e-ed7e3dad7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No kwargs -> type: str\n",
      "  value: '<bos><start_of_turn>user\\nWhat type of scan is this?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThis is a chest X-ray.<end_of_turn>\\n'\n",
      "\n",
      "tokenize=False -> type: str\n",
      "  value: '<bos><start_of_turn>user\\nWhat type of scan is this?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThis is a chest X-ray.<end_of_turn>\\n'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "convo = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What type of scan is this?\"},\n",
    "        {\"type\": \"image\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"This is a chest X-ray.\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "# what the old code did (no kwargs)\n",
    "try:\n",
    "    result_old = processor.apply_chat_template(convo)\n",
    "    print(f\"No kwargs -> type: {type(result_old).__name__}\")\n",
    "    if isinstance(result_old, str):\n",
    "        print(f\"  value: {repr(result_old[:200])}\")\n",
    "    elif hasattr(result_old, 'shape'):\n",
    "        print(f\"  shape: {result_old.shape}\")\n",
    "    elif isinstance(result_old, list):\n",
    "        print(f\"  len: {len(result_old)}, first few: {result_old[:10]}\")\n",
    "    else:\n",
    "        print(f\"  value: {result_old}\")\n",
    "except Exception as e:\n",
    "    print(f\"No kwargs -> EXCEPTION: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# what the new code does\n",
    "result_new = processor.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "print(f\"tokenize=False -> type: {type(result_new).__name__}\")\n",
    "print(f\"  value: {repr(result_new[:200])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0369e2b0-a102-430d-b8f2-17228431be7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llava_med_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllava_med_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLaVAMedDataset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'llava_med_dataset'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from llava_med_dataset import LLaVAMedDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc14b3-6823-4522-90ef-feef1f355703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f710d30-8516-42c3-bbb0-29948a0ef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8fcb49a-c70c-41d9-91f4-5fac3b135147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup done\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# fake test image\n",
    "fake_image = Image.new(\"RGB\", (224, 224), color=(128, 64, 32))\n",
    "\n",
    "# single-turn conversation matching your actual data format\n",
    "single_turn = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is the purpose of the flow diagram?\"},\n",
    "        {\"type\": \"image\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"The purpose of the flow diagram is to illustrate the lung cancer screening process.\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "# multi-turn conversation matching your actual data format\n",
    "multi_turn = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is the purpose of the flow diagram?\"},\n",
    "        {\"type\": \"image\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"The purpose of the flow diagram is to illustrate the lung cancer screening process.\"},\n",
    "    ]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What is the primary screening method?\"},\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"The primary screening method involves chest X-ray examinations.\"},\n",
    "    ]},\n",
    "]\n",
    "\n",
    "print(\"setup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eddfc741-7b61-4fef-9db5-ea89fd51033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single turn ===\n",
      "'<bos><start_of_turn>user\\nWhat is the purpose of the flow diagram?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThe purpose of the flow diagram is to illustrate the lung cancer screening process.<end_of_turn>\\n'\n",
      "\n",
      "=== Multi turn ===\n",
      "'<bos><start_of_turn>user\\nWhat is the purpose of the flow diagram?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThe purpose of the flow diagram is to illustrate the lung cancer screening process.<end_of_turn>\\n<start_of_turn>user\\nWhat is the primary screening method?<end_of_turn>\\n<start_of_turn>model\\nThe primary screening method involves chest X-ray examinations.<end_of_turn>\\n'\n",
      "\n",
      "apply_chat_template: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "\n",
    "# single turn\n",
    "text_single = processor.apply_chat_template(\n",
    "    single_turn, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(\"=== Single turn ===\")\n",
    "print(repr(text_single))\n",
    "print()\n",
    "\n",
    "# multi turn\n",
    "text_multi = processor.apply_chat_template(\n",
    "    multi_turn, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(\"=== Multi turn ===\")\n",
    "print(repr(text_multi))\n",
    "print()\n",
    "\n",
    "# structural checks\n",
    "assert \"<start_of_turn>user\" in text_single\n",
    "assert \"<start_of_turn>model\" in text_single\n",
    "assert \"<end_of_turn>\" in text_single\n",
    "assert \"flow diagram\" in text_single\n",
    "assert \"lung cancer screening\" in text_single\n",
    "\n",
    "# multi-turn should have 2 model turns\n",
    "assert text_multi.count(\"<start_of_turn>model\") == 2, \\\n",
    "    f\"expected 2 model turns, got {text_multi.count('<start_of_turn>model')}\"\n",
    "assert text_multi.count(\"<start_of_turn>user\") == 2\n",
    "\n",
    "print(\"apply_chat_template: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64dc61f2-802f-4c4b-a044-feef2741eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values']\n",
      "input_ids shape:     torch.Size([1, 296])\n",
      "attention_mask shape: torch.Size([1, 296])\n",
      "pixel_values shape:  torch.Size([1, 3, 896, 896])\n",
      "\n",
      "Decoded (first 500 chars):\n",
      "<bos><bos><start_of_turn>user\n",
      "What is the purpose of the flow diagram?\n",
      "\n",
      "<start_of_image><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_toke\n",
      "\n",
      "two-step processor: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    single_turn, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "\n",
    "batch = processor(\n",
    "    text=[text],\n",
    "    images=[fake_image],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(f\"Batch keys: {list(batch.keys())}\")\n",
    "print(f\"input_ids shape:     {batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "print(f\"pixel_values shape:  {batch['pixel_values'].shape}\")\n",
    "\n",
    "assert \"input_ids\" in batch\n",
    "assert \"attention_mask\" in batch\n",
    "assert \"pixel_values\" in batch\n",
    "\n",
    "decoded = processor.tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=False)\n",
    "print(f\"\\nDecoded (first 500 chars):\\n{decoded[:500]}\")\n",
    "\n",
    "print(\"\\ntwo-step processor: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a288cd2-a177-4b5d-8071-f05dd048bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Token ID map ===\n",
      "  <start_of_turn>      ->      105  OK\n",
      "  <end_of_turn>        ->      106  OK\n",
      "  model                ->     4368  OK\n",
      "  user                 ->     2364  OK\n",
      "  image_token_id       ->   262144\n",
      "\n",
      "token IDs: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "tokenizer = processor.tokenizer\n",
    "unk_id = tokenizer.convert_tokens_to_ids(\"<unk>\")\n",
    "\n",
    "tokens_to_check = {\n",
    "    \"<start_of_turn>\": tokenizer.convert_tokens_to_ids(\"<start_of_turn>\"),\n",
    "    \"<end_of_turn>\":   tokenizer.convert_tokens_to_ids(\"<end_of_turn>\"),\n",
    "    \"model\":           tokenizer.convert_tokens_to_ids(\"model\"),\n",
    "    \"user\":            tokenizer.convert_tokens_to_ids(\"user\"),\n",
    "}\n",
    "\n",
    "print(\"=== Token ID map ===\")\n",
    "for name, tid in tokens_to_check.items():\n",
    "    status = \"OK\" if tid != unk_id else \"MISSING (resolved to UNK!)\"\n",
    "    print(f\"  {name:20s} -> {tid:8d}  {status}\")\n",
    "    assert tid != unk_id, f\"{name} resolved to UNK — not in vocab!\"\n",
    "\n",
    "# also check image token\n",
    "if hasattr(processor, \"image_token_id\"):\n",
    "    print(f\"  {'image_token_id':20s} -> {processor.image_token_id:8d}\")\n",
    "else:\n",
    "    print(\"  WARNING: processor has no image_token_id attribute\")\n",
    "\n",
    "print(\"\\ntoken IDs: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a99d8c1-4dfd-4ffb-98e8-b8560cd3be6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Token-level label mask (multi-turn) ===\n",
      "  [  0] <bos>                           -----\n",
      "  [  1] <bos>                           -----\n",
      "  [  2] <start_of_turn>                 -----\n",
      "  [  3] user                            -----\n",
      "  [  4] \n",
      "                               -----\n",
      "  [  5] What                            -----\n",
      "  [  6] ▁is                             -----\n",
      "  [  7] ▁the                            -----\n",
      "  [  8] ▁purpose                        -----\n",
      "  [  9] ▁of                             -----\n",
      "  [ 10] ▁the                            -----\n",
      "  [ 11] ▁flow                           -----\n",
      "  [ 12] ▁diagram                        -----\n",
      "  [ 13] ?                               -----\n",
      "  [ 14] \n",
      "\n",
      "                              -----\n",
      "  [ 15] <start_of_image>                -----\n",
      "  [ 16] <image_soft_token>              -----\n",
      "  [ 17] <image_soft_token>              -----\n",
      "  [ 18] <image_soft_token>              -----\n",
      "  [ 19] <image_soft_token>              -----\n",
      "  [ 20] <image_soft_token>              -----\n",
      "  [ 21] <image_soft_token>              -----\n",
      "  [ 22] <image_soft_token>              -----\n",
      "  [ 23] <image_soft_token>              -----\n",
      "  [ 24] <image_soft_token>              -----\n",
      "  [ 25] <image_soft_token>              -----\n",
      "  [ 26] <image_soft_token>              -----\n",
      "  [ 27] <image_soft_token>              -----\n",
      "  [ 28] <image_soft_token>              -----\n",
      "  [ 29] <image_soft_token>              -----\n",
      "  [ 30] <image_soft_token>              -----\n",
      "  [ 31] <image_soft_token>              -----\n",
      "  [ 32] <image_soft_token>              -----\n",
      "  [ 33] <image_soft_token>              -----\n",
      "  [ 34] <image_soft_token>              -----\n",
      "  [ 35] <image_soft_token>              -----\n",
      "  [ 36] <image_soft_token>              -----\n",
      "  [ 37] <image_soft_token>              -----\n",
      "  [ 38] <image_soft_token>              -----\n",
      "  [ 39] <image_soft_token>              -----\n",
      "  [ 40] <image_soft_token>              -----\n",
      "  [ 41] <image_soft_token>              -----\n",
      "  [ 42] <image_soft_token>              -----\n",
      "  [ 43] <image_soft_token>              -----\n",
      "  [ 44] <image_soft_token>              -----\n",
      "  [ 45] <image_soft_token>              -----\n",
      "  [ 46] <image_soft_token>              -----\n",
      "  [ 47] <image_soft_token>              -----\n",
      "  [ 48] <image_soft_token>              -----\n",
      "  [ 49] <image_soft_token>              -----\n",
      "  [ 50] <image_soft_token>              -----\n",
      "  [ 51] <image_soft_token>              -----\n",
      "  [ 52] <image_soft_token>              -----\n",
      "  [ 53] <image_soft_token>              -----\n",
      "  [ 54] <image_soft_token>              -----\n",
      "  [ 55] <image_soft_token>              -----\n",
      "  [ 56] <image_soft_token>              -----\n",
      "  [ 57] <image_soft_token>              -----\n",
      "  [ 58] <image_soft_token>              -----\n",
      "  [ 59] <image_soft_token>              -----\n",
      "  [ 60] <image_soft_token>              -----\n",
      "  [ 61] <image_soft_token>              -----\n",
      "  [ 62] <image_soft_token>              -----\n",
      "  [ 63] <image_soft_token>              -----\n",
      "  [ 64] <image_soft_token>              -----\n",
      "  [ 65] <image_soft_token>              -----\n",
      "  [ 66] <image_soft_token>              -----\n",
      "  [ 67] <image_soft_token>              -----\n",
      "  [ 68] <image_soft_token>              -----\n",
      "  [ 69] <image_soft_token>              -----\n",
      "  [ 70] <image_soft_token>              -----\n",
      "  [ 71] <image_soft_token>              -----\n",
      "  [ 72] <image_soft_token>              -----\n",
      "  [ 73] <image_soft_token>              -----\n",
      "  [ 74] <image_soft_token>              -----\n",
      "  [ 75] <image_soft_token>              -----\n",
      "  [ 76] <image_soft_token>              -----\n",
      "  [ 77] <image_soft_token>              -----\n",
      "  [ 78] <image_soft_token>              -----\n",
      "  [ 79] <image_soft_token>              -----\n",
      "  [ 80] <image_soft_token>              -----\n",
      "  [ 81] <image_soft_token>              -----\n",
      "  [ 82] <image_soft_token>              -----\n",
      "  [ 83] <image_soft_token>              -----\n",
      "  [ 84] <image_soft_token>              -----\n",
      "  [ 85] <image_soft_token>              -----\n",
      "  [ 86] <image_soft_token>              -----\n",
      "  [ 87] <image_soft_token>              -----\n",
      "  [ 88] <image_soft_token>              -----\n",
      "  [ 89] <image_soft_token>              -----\n",
      "  [ 90] <image_soft_token>              -----\n",
      "  [ 91] <image_soft_token>              -----\n",
      "  [ 92] <image_soft_token>              -----\n",
      "  [ 93] <image_soft_token>              -----\n",
      "  [ 94] <image_soft_token>              -----\n",
      "  [ 95] <image_soft_token>              -----\n",
      "  [ 96] <image_soft_token>              -----\n",
      "  [ 97] <image_soft_token>              -----\n",
      "  [ 98] <image_soft_token>              -----\n",
      "  [ 99] <image_soft_token>              -----\n",
      "  [100] <image_soft_token>              -----\n",
      "  [101] <image_soft_token>              -----\n",
      "  [102] <image_soft_token>              -----\n",
      "  [103] <image_soft_token>              -----\n",
      "  [104] <image_soft_token>              -----\n",
      "  [105] <image_soft_token>              -----\n",
      "  [106] <image_soft_token>              -----\n",
      "  [107] <image_soft_token>              -----\n",
      "  [108] <image_soft_token>              -----\n",
      "  [109] <image_soft_token>              -----\n",
      "  [110] <image_soft_token>              -----\n",
      "  [111] <image_soft_token>              -----\n",
      "  [112] <image_soft_token>              -----\n",
      "  [113] <image_soft_token>              -----\n",
      "  [114] <image_soft_token>              -----\n",
      "  [115] <image_soft_token>              -----\n",
      "  [116] <image_soft_token>              -----\n",
      "  [117] <image_soft_token>              -----\n",
      "  [118] <image_soft_token>              -----\n",
      "  [119] <image_soft_token>              -----\n",
      "  [120] <image_soft_token>              -----\n",
      "  [121] <image_soft_token>              -----\n",
      "  [122] <image_soft_token>              -----\n",
      "  [123] <image_soft_token>              -----\n",
      "  [124] <image_soft_token>              -----\n",
      "  [125] <image_soft_token>              -----\n",
      "  [126] <image_soft_token>              -----\n",
      "  [127] <image_soft_token>              -----\n",
      "  [128] <image_soft_token>              -----\n",
      "  [129] <image_soft_token>              -----\n",
      "  [130] <image_soft_token>              -----\n",
      "  [131] <image_soft_token>              -----\n",
      "  [132] <image_soft_token>              -----\n",
      "  [133] <image_soft_token>              -----\n",
      "  [134] <image_soft_token>              -----\n",
      "  [135] <image_soft_token>              -----\n",
      "  [136] <image_soft_token>              -----\n",
      "  [137] <image_soft_token>              -----\n",
      "  [138] <image_soft_token>              -----\n",
      "  [139] <image_soft_token>              -----\n",
      "  [140] <image_soft_token>              -----\n",
      "  [141] <image_soft_token>              -----\n",
      "  [142] <image_soft_token>              -----\n",
      "  [143] <image_soft_token>              -----\n",
      "  [144] <image_soft_token>              -----\n",
      "  [145] <image_soft_token>              -----\n",
      "  [146] <image_soft_token>              -----\n",
      "  [147] <image_soft_token>              -----\n",
      "  [148] <image_soft_token>              -----\n",
      "  [149] <image_soft_token>              -----\n",
      "  [150] <image_soft_token>              -----\n",
      "  [151] <image_soft_token>              -----\n",
      "  [152] <image_soft_token>              -----\n",
      "  [153] <image_soft_token>              -----\n",
      "  [154] <image_soft_token>              -----\n",
      "  [155] <image_soft_token>              -----\n",
      "  [156] <image_soft_token>              -----\n",
      "  [157] <image_soft_token>              -----\n",
      "  [158] <image_soft_token>              -----\n",
      "  [159] <image_soft_token>              -----\n",
      "  [160] <image_soft_token>              -----\n",
      "  [161] <image_soft_token>              -----\n",
      "  [162] <image_soft_token>              -----\n",
      "  [163] <image_soft_token>              -----\n",
      "  [164] <image_soft_token>              -----\n",
      "  [165] <image_soft_token>              -----\n",
      "  [166] <image_soft_token>              -----\n",
      "  [167] <image_soft_token>              -----\n",
      "  [168] <image_soft_token>              -----\n",
      "  [169] <image_soft_token>              -----\n",
      "  [170] <image_soft_token>              -----\n",
      "  [171] <image_soft_token>              -----\n",
      "  [172] <image_soft_token>              -----\n",
      "  [173] <image_soft_token>              -----\n",
      "  [174] <image_soft_token>              -----\n",
      "  [175] <image_soft_token>              -----\n",
      "  [176] <image_soft_token>              -----\n",
      "  [177] <image_soft_token>              -----\n",
      "  [178] <image_soft_token>              -----\n",
      "  [179] <image_soft_token>              -----\n",
      "  [180] <image_soft_token>              -----\n",
      "  [181] <image_soft_token>              -----\n",
      "  [182] <image_soft_token>              -----\n",
      "  [183] <image_soft_token>              -----\n",
      "  [184] <image_soft_token>              -----\n",
      "  [185] <image_soft_token>              -----\n",
      "  [186] <image_soft_token>              -----\n",
      "  [187] <image_soft_token>              -----\n",
      "  [188] <image_soft_token>              -----\n",
      "  [189] <image_soft_token>              -----\n",
      "  [190] <image_soft_token>              -----\n",
      "  [191] <image_soft_token>              -----\n",
      "  [192] <image_soft_token>              -----\n",
      "  [193] <image_soft_token>              -----\n",
      "  [194] <image_soft_token>              -----\n",
      "  [195] <image_soft_token>              -----\n",
      "  [196] <image_soft_token>              -----\n",
      "  [197] <image_soft_token>              -----\n",
      "  [198] <image_soft_token>              -----\n",
      "  [199] <image_soft_token>              -----\n",
      "  [200] <image_soft_token>              -----\n",
      "  [201] <image_soft_token>              -----\n",
      "  [202] <image_soft_token>              -----\n",
      "  [203] <image_soft_token>              -----\n",
      "  [204] <image_soft_token>              -----\n",
      "  [205] <image_soft_token>              -----\n",
      "  [206] <image_soft_token>              -----\n",
      "  [207] <image_soft_token>              -----\n",
      "  [208] <image_soft_token>              -----\n",
      "  [209] <image_soft_token>              -----\n",
      "  [210] <image_soft_token>              -----\n",
      "  [211] <image_soft_token>              -----\n",
      "  [212] <image_soft_token>              -----\n",
      "  [213] <image_soft_token>              -----\n",
      "  [214] <image_soft_token>              -----\n",
      "  [215] <image_soft_token>              -----\n",
      "  [216] <image_soft_token>              -----\n",
      "  [217] <image_soft_token>              -----\n",
      "  [218] <image_soft_token>              -----\n",
      "  [219] <image_soft_token>              -----\n",
      "  [220] <image_soft_token>              -----\n",
      "  [221] <image_soft_token>              -----\n",
      "  [222] <image_soft_token>              -----\n",
      "  [223] <image_soft_token>              -----\n",
      "  [224] <image_soft_token>              -----\n",
      "  [225] <image_soft_token>              -----\n",
      "  [226] <image_soft_token>              -----\n",
      "  [227] <image_soft_token>              -----\n",
      "  [228] <image_soft_token>              -----\n",
      "  [229] <image_soft_token>              -----\n",
      "  [230] <image_soft_token>              -----\n",
      "  [231] <image_soft_token>              -----\n",
      "  [232] <image_soft_token>              -----\n",
      "  [233] <image_soft_token>              -----\n",
      "  [234] <image_soft_token>              -----\n",
      "  [235] <image_soft_token>              -----\n",
      "  [236] <image_soft_token>              -----\n",
      "  [237] <image_soft_token>              -----\n",
      "  [238] <image_soft_token>              -----\n",
      "  [239] <image_soft_token>              -----\n",
      "  [240] <image_soft_token>              -----\n",
      "  [241] <image_soft_token>              -----\n",
      "  [242] <image_soft_token>              -----\n",
      "  [243] <image_soft_token>              -----\n",
      "  [244] <image_soft_token>              -----\n",
      "  [245] <image_soft_token>              -----\n",
      "  [246] <image_soft_token>              -----\n",
      "  [247] <image_soft_token>              -----\n",
      "  [248] <image_soft_token>              -----\n",
      "  [249] <image_soft_token>              -----\n",
      "  [250] <image_soft_token>              -----\n",
      "  [251] <image_soft_token>              -----\n",
      "  [252] <image_soft_token>              -----\n",
      "  [253] <image_soft_token>              -----\n",
      "  [254] <image_soft_token>              -----\n",
      "  [255] <image_soft_token>              -----\n",
      "  [256] <image_soft_token>              -----\n",
      "  [257] <image_soft_token>              -----\n",
      "  [258] <image_soft_token>              -----\n",
      "  [259] <image_soft_token>              -----\n",
      "  [260] <image_soft_token>              -----\n",
      "  [261] <image_soft_token>              -----\n",
      "  [262] <image_soft_token>              -----\n",
      "  [263] <image_soft_token>              -----\n",
      "  [264] <image_soft_token>              -----\n",
      "  [265] <image_soft_token>              -----\n",
      "  [266] <image_soft_token>              -----\n",
      "  [267] <image_soft_token>              -----\n",
      "  [268] <image_soft_token>              -----\n",
      "  [269] <image_soft_token>              -----\n",
      "  [270] <image_soft_token>              -----\n",
      "  [271] <image_soft_token>              -----\n",
      "  [272] <end_of_image>                  -----\n",
      "  [273] \n",
      "\n",
      "                              -----\n",
      "  [274] <end_of_turn>                   -----\n",
      "  [275] \n",
      "                               -----\n",
      "  [276] <start_of_turn>                 TRAIN\n",
      "  [277] model                           TRAIN\n",
      "  [278] \n",
      "                               TRAIN\n",
      "  [279] The                             TRAIN\n",
      "  [280] ▁purpose                        TRAIN\n",
      "  [281] ▁of                             TRAIN\n",
      "  [282] ▁the                            TRAIN\n",
      "  [283] ▁flow                           TRAIN\n",
      "  [284] ▁diagram                        TRAIN\n",
      "  [285] ▁is                             TRAIN\n",
      "  [286] ▁to                             TRAIN\n",
      "  [287] ▁illustrate                     TRAIN\n",
      "  [288] ▁the                            TRAIN\n",
      "  [289] ▁lung                           TRAIN\n",
      "  [290] ▁cancer                         TRAIN\n",
      "  [291] ▁screening                      TRAIN\n",
      "  [292] ▁process                        TRAIN\n",
      "  [293] .                               TRAIN\n",
      "  [294] <end_of_turn>                   TRAIN\n",
      "  [295] \n",
      "                               -----\n",
      "  [296] <start_of_turn>                 -----\n",
      "  [297] user                            -----\n",
      "  [298] \n",
      "                               -----\n",
      "  [299] What                            -----\n",
      "  [300] ▁is                             -----\n",
      "  [301] ▁the                            -----\n",
      "  [302] ▁primary                        -----\n",
      "  [303] ▁screening                      -----\n",
      "  [304] ▁method                         -----\n",
      "  [305] ?                               -----\n",
      "  [306] <end_of_turn>                   -----\n",
      "  [307] \n",
      "                               -----\n",
      "  [308] <start_of_turn>                 TRAIN\n",
      "  [309] model                           TRAIN\n",
      "  [310] \n",
      "                               TRAIN\n",
      "  [311] The                             TRAIN\n",
      "  [312] ▁primary                        TRAIN\n",
      "  [313] ▁screening                      TRAIN\n",
      "  [314] ▁method                         TRAIN\n",
      "  [315] ▁involves                       TRAIN\n",
      "  [316] ▁chest                          TRAIN\n",
      "  [317] ▁X                              TRAIN\n",
      "  [318] -                               TRAIN\n",
      "  [319] ray                             TRAIN\n",
      "  [320] ▁examinations                   TRAIN\n",
      "  [321] .                               TRAIN\n",
      "  [322] <end_of_turn>                   TRAIN\n",
      "  [323] \n",
      "                               -----\n",
      "\n",
      "Trained text:\n",
      "<start_of_turn>model\n",
      "The purpose of the flow diagram is to illustrate the lung cancer screening process.<end_of_turn><start_of_turn>model\n",
      "The primary screening method involves chest X-ray examinations.<end_of_turn>\n",
      "\n",
      "IFT masking: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "# tokenize the multi-turn sample\n",
    "text = processor.apply_chat_template(\n",
    "    multi_turn, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "batch = processor(\n",
    "    text=[text], images=[fake_image],\n",
    "    padding=True, truncation=True, max_length=512, return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "start_of_turn_id = tokenizer.convert_tokens_to_ids(\"<start_of_turn>\")\n",
    "end_of_turn_id = tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "model_token_id = tokenizer.convert_tokens_to_ids(\"model\")\n",
    "\n",
    "input_ids = batch[\"input_ids\"]\n",
    "labels = torch.full_like(input_ids, fill_value=-100)\n",
    "\n",
    "in_model_turn = False\n",
    "for j in range(input_ids.shape[1]):\n",
    "    tok = input_ids[0, j].item()\n",
    "    if tok == start_of_turn_id:\n",
    "        in_model_turn = False\n",
    "        if j + 1 < input_ids.shape[1]:\n",
    "            if input_ids[0, j + 1].item() == model_token_id:\n",
    "                in_model_turn = True\n",
    "    if in_model_turn:\n",
    "        labels[0, j] = input_ids[0, j]\n",
    "    if tok == end_of_turn_id and in_model_turn:\n",
    "        in_model_turn = False\n",
    "\n",
    "labels[input_ids == tokenizer.pad_token_id] = -100\n",
    "if hasattr(processor, \"image_token_id\"):\n",
    "    labels[input_ids == processor.image_token_id] = -100\n",
    "\n",
    "# print token-by-token mask\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "print(\"=== Token-level label mask (multi-turn) ===\")\n",
    "for j, (tok, lab) in enumerate(zip(tokens, labels[0].tolist())):\n",
    "    if tok not in [\"<pad>\"]:\n",
    "        status = \"TRAIN\" if lab != -100 else \"-----\"\n",
    "        print(f\"  [{j:3d}] {tok:30s}  {status}\")\n",
    "\n",
    "# verify: assistant content should be trained, user content should not\n",
    "trained_text = tokenizer.decode(\n",
    "    [t for t, l in zip(input_ids[0].tolist(), labels[0].tolist()) if l != -100]\n",
    ")\n",
    "print(f\"\\nTrained text:\\n{trained_text}\")\n",
    "\n",
    "assert \"lung cancer screening\" in trained_text, \"first assistant response should be trained\"\n",
    "assert \"chest X-ray\" in trained_text, \"second assistant response should be trained\"\n",
    "\n",
    "# user questions should NOT appear in trained text\n",
    "assert \"What is the purpose of\" not in trained_text, \"user question should be masked\"\n",
    "assert \"What is the primary\" not in trained_text, \"second user question should be masked\"\n",
    "\n",
    "print(\"\\nIFT masking: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a17095f5-61f7-43c6-9cb6-8e1f14688289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<PIL.Image.Image image mode=RGB size=256x256 at 0x155393A9A840>], [<PIL.Image.Image image mode=RGB size=256x256 at 0x155393A9A090>], [<PIL.Image.Image image mode=RGB size=300x200 at 0x155393A999A0>]]\n",
      "['<bos><start_of_turn>user\\nWhat type of scan is this?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThis is an axial CT scan of the abdomen.<end_of_turn>\\n', '<bos><start_of_turn>user\\nDescribe the pathology.<start_of_image><end_of_turn>\\n<start_of_turn>model\\nThere is a large mass in the left frontal lobe consistent with a glioma.<end_of_turn>\\n', '<bos><start_of_turn>user\\nWhat do you see?<start_of_image><end_of_turn>\\n<start_of_turn>model\\nNormal chest radiograph with no acute findings.<end_of_turn>\\n']\n",
      "=== reg_train_collate_fn ===\n",
      "input_ids:     torch.Size([3, 291])\n",
      "attention_mask: torch.Size([3, 291])\n",
      "pixel_values:  torch.Size([3, 3, 896, 896])\n",
      "labels:        torch.Size([3, 291])\n",
      "\n",
      "=== ift_train_collate_fn ===\n",
      "input_ids: torch.Size([3, 291])\n",
      "labels:    torch.Size([3, 291])\n",
      "\n",
      "reg trained tokens: 98\n",
      "ift trained tokens: 46\n",
      "sample 0: 14 trained tokens\n",
      "sample 1: 19 trained tokens\n",
      "sample 2: 13 trained tokens\n",
      "\n",
      "batch collate: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "from monosemanticity.datasets.base_multimodal_dataset import BaseMultimodalDataModule\n",
    "\n",
    "# fake samples: (PIL_image, conversation_list) — what __getitem__ returns with data_key=\"conversations\"\n",
    "samples = [\n",
    "    (Image.new(\"RGB\", (256, 256), color=(100, 50, 50)), [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What type of scan is this?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ]},\n",
    "        {\"role\": \"assistant\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"This is an axial CT scan of the abdomen.\"},\n",
    "        ]},\n",
    "    ]),\n",
    "    (Image.new(\"RGB\", (256, 256), color=(50, 100, 50)), [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the pathology.\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ]},\n",
    "        {\"role\": \"assistant\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"There is a large mass in the left frontal lobe consistent with a glioma.\"},\n",
    "        ]},\n",
    "    ]),\n",
    "    (Image.new(\"RGB\", (300, 200), color=(50, 50, 100)), [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What do you see?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ]},\n",
    "        {\"role\": \"assistant\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Normal chest radiograph with no acute findings.\"},\n",
    "        ]},\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# minimal DataModule subclass just to get the collate methods\n",
    "class TestModule(BaseMultimodalDataModule):\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "# --- reg_train_collate_fn ---\n",
    "dm = TestModule(root_dir=\"/tmp\", processor=processor, batch_size=3, max_tokens=512, ift=False)\n",
    "reg_batch = dm.reg_train_collate_fn(samples)\n",
    "\n",
    "print(\"=== reg_train_collate_fn ===\")\n",
    "print(f\"input_ids:     {reg_batch['input_ids'].shape}\")\n",
    "print(f\"attention_mask: {reg_batch['attention_mask'].shape}\")\n",
    "print(f\"pixel_values:  {reg_batch['pixel_values'].shape}\")\n",
    "print(f\"labels:        {reg_batch['labels'].shape}\")\n",
    "\n",
    "assert reg_batch[\"input_ids\"].shape[0] == 3\n",
    "assert (reg_batch[\"labels\"][reg_batch[\"input_ids\"] == processor.tokenizer.pad_token_id] == -100).all(), \\\n",
    "    \"pad tokens should be masked\"\n",
    "if hasattr(processor, \"image_token_id\"):\n",
    "    assert (reg_batch[\"labels\"][reg_batch[\"input_ids\"] == processor.image_token_id] == -100).all(), \\\n",
    "        \"image tokens should be masked\"\n",
    "\n",
    "# --- ift_train_collate_fn ---\n",
    "dm_ift = TestModule(root_dir=\"/tmp\", processor=processor, batch_size=3, max_tokens=512, ift=True)\n",
    "ift_batch = dm_ift.ift_train_collate_fn(samples)\n",
    "\n",
    "print(\"\\n=== ift_train_collate_fn ===\")\n",
    "print(f\"input_ids: {ift_batch['input_ids'].shape}\")\n",
    "print(f\"labels:    {ift_batch['labels'].shape}\")\n",
    "\n",
    "reg_trained = (reg_batch[\"labels\"] != -100).sum().item()\n",
    "ift_trained = (ift_batch[\"labels\"] != -100).sum().item()\n",
    "print(f\"\\nreg trained tokens: {reg_trained}\")\n",
    "print(f\"ift trained tokens: {ift_trained}\")\n",
    "assert ift_trained < reg_trained, \"IFT should have fewer trained tokens (user turns masked)\"\n",
    "\n",
    "for i in range(3):\n",
    "    n = (ift_batch[\"labels\"][i] != -100).sum().item()\n",
    "    assert n > 0, f\"sample {i} has 0 trained tokens — masking is broken\"\n",
    "    print(f\"sample {i}: {n} trained tokens\")\n",
    "\n",
    "print(\"\\nbatch collate: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a54bdef-312e-4650-8164-244508c991f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<PIL.Image.Image image mode=RGB size=256x256>,\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text', 'text': 'What type of scan is this?'},\n",
       "     {'type': 'image'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'This is an axial CT scan of the abdomen.'}]}]),\n",
       " (<PIL.Image.Image image mode=RGB size=256x256>,\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text', 'text': 'Describe the pathology.'},\n",
       "     {'type': 'image'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'There is a large mass in the left frontal lobe consistent with a glioma.'}]}]),\n",
       " (<PIL.Image.Image image mode=RGB size=300x200>,\n",
       "  [{'role': 'user',\n",
       "    'content': [{'type': 'text', 'text': 'What do you see?'},\n",
       "     {'type': 'image'}]},\n",
       "   {'role': 'assistant',\n",
       "    'content': [{'type': 'text',\n",
       "      'text': 'Normal chest radiograph with no acute findings.'}]}])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "586a5aef-9de2-4bb5-94cb-bea803df6412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedGemma batch keys: ['input_ids', 'attention_mask', 'token_type_ids', 'pixel_values']\n",
      "input_ids shape: torch.Size([1, 296])\n",
      "MedGemma processor: PASSED (same token IDs as Gemma 3)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    med_processor = AutoProcessor.from_pretrained(\"google/medgemma-4b-it\")\n",
    "\n",
    "    text = med_processor.apply_chat_template(\n",
    "        single_turn, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    batch = med_processor(\n",
    "        text=[text], images=[fake_image], padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # should have the same keys and work the same way\n",
    "    print(f\"MedGemma batch keys: {list(batch.keys())}\")\n",
    "    print(f\"input_ids shape: {batch['input_ids'].shape}\")\n",
    "\n",
    "    # verify same token vocabulary for boundary tokens\n",
    "    med_tok = med_processor.tokenizer\n",
    "    assert med_tok.convert_tokens_to_ids(\"<start_of_turn>\") == \\\n",
    "           processor.tokenizer.convert_tokens_to_ids(\"<start_of_turn>\"), \\\n",
    "        \"start_of_turn token ID mismatch between gemma and medgemma\"\n",
    "\n",
    "    print(\"MedGemma processor: PASSED (same token IDs as Gemma 3)\")\n",
    "except Exception as e:\n",
    "    print(f\"MedGemma not available: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "792465b9-a6e7-4121-9845-d239d4f4a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell — Config (edit these)\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "DATA_ROOT = \"/gpfs/data/oermannlab/public_data/llava-med-data\"\n",
    "DATASET_FILE = \"llava_med_instruct_60k_inline_mention_filtered.json\"\n",
    "MODEL_ID = \"google/gemma-3-4b-it\"  # or \"google/medgemma-4b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4e3d050-77ca-443d-9a0c-2c0a5748f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded: 56658 samples\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Load processor and dataset\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "from datasets.llava_med_dataset import LLaVAMedDataset, LLaVAMedDataModule\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\"\n",
    "\n",
    "# load just the raw dataset (no splits, no collation)\n",
    "ds = LLaVAMedDataset(\n",
    "    root_dir=DATA_ROOT,\n",
    "    dataset_file=DATASET_FILE,\n",
    "    data_key=\"conversations\",\n",
    ")\n",
    "print(f\"dataset loaded: {len(ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97ebe749-9900-4796-a2c8-9cba0eab4da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: (800, 579), mode=L\n",
      "conversation turns: 6\n",
      "  turn 0: role=user  blocks=['text', 'image']  text=What is the main finding in the image?...\n",
      "  turn 1: role=assistant  blocks=['text']  text=The main finding in the image is bilateral lower lobe consolidation, which is pr...\n",
      "  turn 2: role=user  blocks=['text']  text=What does consolidation mean?...\n",
      "  turn 3: role=assistant  blocks=['text']  text=Consolidation refers to a region of the lung where the air spaces (alveoli) are ...\n",
      "  turn 4: role=user  blocks=['text']  text=Is there any other finding mentioned in the context?...\n",
      "  turn 5: role=assistant  blocks=['text']  text=Which is bronchiectasis. bronchiectasis is a chronic lung condition characterize...\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Inspect a single raw sample\n",
    "image, convo = ds[0]\n",
    "print(f\"image: {image.size}, mode={image.mode}\")\n",
    "print(f\"conversation turns: {len(convo)}\")\n",
    "for i, turn in enumerate(convo):\n",
    "    role = turn[\"role\"]\n",
    "    content = turn[\"content\"]\n",
    "    if isinstance(content, list):\n",
    "        types = [c[\"type\"] for c in content]\n",
    "        text = next((c[\"text\"] for c in content if c[\"type\"] == \"text\"), \"\")\n",
    "        print(f\"  turn {i}: role={role}  blocks={types}  text={text[:80]}...\")\n",
    "    else:\n",
    "        print(f\"  turn {i}: role={role}  content={str(content)[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c497666-65b5-4a8c-9f43-794f746ce03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Formatted ===\n",
      "<bos><start_of_turn>user\n",
      "What is the main finding in the image?<start_of_image><end_of_turn>\n",
      "<start_of_turn>model\n",
      "The main finding in the image is bilateral lower lobe consolidation, which is present in both lungs.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "What does consolidation mean?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Consolidation refers to a region of the lung where the air spaces (alveoli) are filled with fluid, pus, blood, or cells, making the lung tissue appear more solid and dense on imaging studies like a computed tomography (CT) scan or X-ray. Consolidation can be caused by various conditions, such as pneumonia, pulmonary edema, or lung injury. It is important to consider the patient's clinical history and symptoms, as well as consult a healthcare professional for a thorough evaluation and proper diagnosis of the underlying cause of the consolidation.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Is there any other finding mentioned in the context?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Which is bronchiectasis. bronchiectasis is a chronic lung condition characterized by the abnormal widening and thickening of the bronchial tubes, leading to impaired mucus clearance and recurrent lung infections. however, this finding is not visible in the provided image, as it is specifically showing bilateral lower lobe consolidation.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Test apply_chat_template on real sample\n",
    "image, convo = ds[0]\n",
    "formatted = processor.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "print(\"=== Formatted ===\")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5ab2132-7b7f-4b92-b020-56c69a46dde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Single sample batch ===\n",
      "  input_ids            torch.Size([1, 502])           dtype=torch.int64\n",
      "  attention_mask       torch.Size([1, 502])           dtype=torch.int64\n",
      "  token_type_ids       torch.Size([1, 502])           dtype=torch.int64\n",
      "  pixel_values         torch.Size([1, 3, 896, 896])   dtype=torch.float32\n",
      "\n",
      "Decoded (first 500 chars):\n",
      "<bos><bos><start_of_turn>user\n",
      "What is the main finding in the image?\n",
      "\n",
      "<start_of_image><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token>\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Test full processor call (single sample)\n",
    "image, convo = ds[0]\n",
    "text = processor.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "batch = processor(\n",
    "    text=[text], images=[image],\n",
    "    padding=True, truncation=True, max_length=4096, return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"=== Single sample batch ===\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"  {k:20s} {str(v.shape):30s} dtype={v.dtype}\")\n",
    "\n",
    "# decode and verify roundtrip\n",
    "decoded = processor.tokenizer.decode(batch[\"input_ids\"][0], skip_special_tokens=False)\n",
    "print(f\"\\nDecoded (first 500 chars):\\n{decoded[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d41e525-cf57-49c1-91bc-3dd01e3a8161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IFT collate, batch_size=1 ===\n",
      "  input_ids            torch.Size([1, 502])           dtype=torch.int64\n",
      "  attention_mask       torch.Size([1, 502])           dtype=torch.int64\n",
      "  token_type_ids       torch.Size([1, 502])           dtype=torch.int64\n",
      "  pixel_values         torch.Size([1, 3, 896, 896])   dtype=torch.float32\n",
      "  labels               torch.Size([1, 502])           dtype=torch.int64\n",
      "\n",
      "  non-pad tokens: 502\n",
      "  trained tokens: 198\n",
      "  masked tokens:  304\n",
      "  trained ratio:  39.4%\n",
      "\n",
      "=== Trained text ===\n",
      "<start_of_turn>model\n",
      "The main finding in the image is bilateral lower lobe consolidation, which is present in both lungs.<end_of_turn><start_of_turn>model\n",
      "Consolidation refers to a region of the lung where the air spaces (alveoli) are filled with fluid, pus, blood, or cells, making the lung tissue appear more solid and dense on imaging studies like a computed tomography (CT) scan or X-ray. Consolidation can be caused by various conditions, such as pneumonia, pulmonary edema, or lung injury. It i\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Test batch of 1 through actual DataModule collate\n",
    "from datasets.base_multimodal_dataset import BaseMultimodalDataModule\n",
    "\n",
    "class TestModule(BaseMultimodalDataModule):\n",
    "    def setup(self, stage=None):\n",
    "        pass\n",
    "\n",
    "dm = TestModule(\n",
    "    root_dir=DATA_ROOT,\n",
    "    processor=processor,\n",
    "    batch_size=1,\n",
    "    max_tokens=4096,\n",
    "    ift=True,\n",
    ")\n",
    "\n",
    "# grab one sample, wrap in list (that's what collate receives)\n",
    "sample = [ds[0]]\n",
    "batch = dm.ift_train_collate_fn(sample)\n",
    "\n",
    "print(\"=== IFT collate, batch_size=1 ===\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"  {k:20s} {str(v.shape):30s} dtype={v.dtype}\")\n",
    "\n",
    "n_trained = (batch[\"labels\"] != -100).sum().item()\n",
    "n_total = (batch[\"input_ids\"] != processor.tokenizer.pad_token_id).sum().item()\n",
    "print(f\"\\n  non-pad tokens: {n_total}\")\n",
    "print(f\"  trained tokens: {n_trained}\")\n",
    "print(f\"  masked tokens:  {n_total - n_trained}\")\n",
    "print(f\"  trained ratio:  {n_trained / n_total:.1%}\")\n",
    "\n",
    "# show what's being trained\n",
    "trained_text = processor.tokenizer.decode(\n",
    "    [t for t, l in zip(batch[\"input_ids\"][0].tolist(), batch[\"labels\"][0].tolist()) if l != -100]\n",
    ")\n",
    "print(f\"\\n=== Trained text ===\\n{trained_text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "666a5571-bbbe-4b53-8d26-00002550284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IFT collate, batch_size=4 ===\n",
      "  input_ids            torch.Size([4, 557])           dtype=torch.int64\n",
      "  attention_mask       torch.Size([4, 557])           dtype=torch.int64\n",
      "  token_type_ids       torch.Size([4, 557])           dtype=torch.int64\n",
      "  pixel_values         torch.Size([4, 3, 896, 896])   dtype=torch.float32\n",
      "  labels               torch.Size([4, 557])           dtype=torch.int64\n",
      "  sample 0: seq_len=557  pad=55  non-pad=502  trained=198  ratio=39.4%\n",
      "  sample 1: seq_len=557  pad=3  non-pad=554  trained=244  ratio=44.0%\n",
      "  sample 2: seq_len=557  pad=0  non-pad=557  trained=243  ratio=43.6%\n",
      "  sample 3: seq_len=557  pad=113  non-pad=444  trained=139  ratio=31.3%\n",
      "\n",
      "  total batch trained tokens: 824\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Test batch of 4 through actual DataModule collate\n",
    "samples = [ds[i] for i in range(4)]\n",
    "\n",
    "batch = dm.ift_train_collate_fn(samples)\n",
    "\n",
    "print(\"=== IFT collate, batch_size=4 ===\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"  {k:20s} {str(v.shape):30s} dtype={v.dtype}\")\n",
    "\n",
    "for i in range(4):\n",
    "    n_trained = (batch[\"labels\"][i] != -100).sum().item()\n",
    "    n_total = (batch[\"input_ids\"][i] != processor.tokenizer.pad_token_id).sum().item()\n",
    "    seq_len = batch[\"input_ids\"].shape[1]\n",
    "    n_pad = (batch[\"input_ids\"][i] == processor.tokenizer.pad_token_id).sum().item()\n",
    "    print(f\"  sample {i}: seq_len={seq_len}  pad={n_pad}  non-pad={n_total}  trained={n_trained}  ratio={n_trained/max(n_total,1):.1%}\")\n",
    "\n",
    "print(f\"\\n  total batch trained tokens: {(batch['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a66d0fc-c764-4555-9081-d5ec10b958b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 45328  val: 5664  test: 5664\n",
      "\n",
      "=== First batch from DataLoader ===\n",
      "  input_ids            torch.Size([4, 563])           dtype=torch.int64\n",
      "  attention_mask       torch.Size([4, 563])           dtype=torch.int64\n",
      "  token_type_ids       torch.Size([4, 563])           dtype=torch.int64\n",
      "  pixel_values         torch.Size([4, 3, 896, 896])   dtype=torch.float32\n",
      "  labels               torch.Size([4, 563])           dtype=torch.int64\n",
      "  sample 0: non-pad=455  trained=148\n",
      "  sample 1: non-pad=462  trained=154\n",
      "  sample 2: non-pad=509  trained=203\n",
      "  sample 3: non-pad=563  trained=259\n",
      "\n",
      "FULL INTEGRATION: PASSED\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Test through actual DataLoader (full integration)\n",
    "dm_full = LLaVAMedDataModule(\n",
    "    DATA_ROOT,\n",
    "    processor,\n",
    "    dataset_file=DATASET_FILE,\n",
    "    data_key=\"conversations\",\n",
    "    batch_size=4,\n",
    "    max_tokens=4096,\n",
    "    ift=True,\n",
    ")\n",
    "dm_full.setup()\n",
    "\n",
    "print(f\"train: {len(dm_full.train_dataset)}  val: {len(dm_full.val_dataset)}  test: {len(dm_full.test_dataset)}\")\n",
    "\n",
    "train_dl = dm_full.train_dataloader()\n",
    "\n",
    "# grab first batch from the actual dataloader\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "print(f\"\\n=== First batch from DataLoader ===\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"  {k:20s} {str(v.shape):30s} dtype={v.dtype}\")\n",
    "\n",
    "for i in range(4):\n",
    "    n_trained = (batch[\"labels\"][i] != -100).sum().item()\n",
    "    n_total = (batch[\"input_ids\"][i] != processor.tokenizer.pad_token_id).sum().item()\n",
    "    print(f\"  sample {i}: non-pad={n_total}  trained={n_trained}\")\n",
    "\n",
    "print(\"\\nFULL INTEGRATION: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07dd226b-60fd-4f01-8d12-9c0e5af614dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What is the main finding in the image?\n",
      "\n",
      "<start_of_image><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><end_of_image>\n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The main finding in the image is bilateral lower lobe consolidation, which is present in both lungs.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "What does consolidation mean?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Consolidation refers to a region of the lung where the air spaces (alveoli) are filled with fluid, pus, blood, or cells, making the lung tissue appear more solid and dense on imaging studies like a computed tomography (CT) scan or X-ray. Consolidation can be caused by various conditions, such as pneumonia, pulmonary edema, or lung injury. It is important to consider the patient's clinical history and symptoms, as well as consult a healthcare professional for a thorough evaluation and proper diagnosis of the underlying cause of the consolidation.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Is there any other finding mentioned in the context?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Which is bronchiectasis. bronchiectasis is a chronic lung condition characterized by the abnormal widening and thickening of the bronchial tubes, leading to impaired mucus clearance and recurrent lung infections. however, this finding is not visible in the provided image, as it is specifically showing bilateral lower lobe consolidation.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if this removes the double <bos>\n",
    "batch = processor(\n",
    "    text=[text],\n",
    "    images=[image],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,  # <-- try this\n",
    ")\n",
    "decoded = processor.tokenizer.decode(batch[\"input_ids\"][0][:], skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fc2a542-3163-4dbb-9f12-339ff5119c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><bos><start_of_turn>user\n",
      "What is the main finding in the image?\n",
      "\n",
      "<start_of_image><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><image_soft_token><end_of_image>\n",
      "\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The main finding in the image is bilateral lower lobe consolidation, which is present in both lungs.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "What does consolidation mean?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Consolidation refers to a region of the lung where the air spaces (alveoli) are filled with fluid, pus, blood, or cells, making the lung tissue appear more solid and dense on imaging studies like a computed tomography (CT) scan or X-ray. Consolidation can be caused by various conditions, such as pneumonia, pulmonary edema, or lung injury. It is important to consider the patient's clinical history and symptoms, as well as consult a healthcare professional for a thorough evaluation and proper diagnosis of the underlying cause of the consolidation.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Is there any other finding mentioned in the context?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Which is bronchiectasis. bronchiectasis is a chronic lung condition characterized by the abnormal widening and thickening of the bronchial tubes, leading to impaired mucus clearance and recurrent lung infections. however, this finding is not visible in the provided image, as it is specifically showing bilateral lower lobe consolidation.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if this removes the double <bos>\n",
    "batch = processor(\n",
    "    text=[text],\n",
    "    images=[image],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True,  # <-- try this\n",
    ")\n",
    "decoded = processor.tokenizer.decode(batch[\"input_ids\"][0][:], skip_special_tokens=False)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d72ee35d-84c2-476a-b267-939be31eab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 4 multi-turn samples\n",
      "  sample 0: 6 turns (3 user, 3 assistant), image=True, img_size=(800, 579)\n",
      "  sample 1: 6 turns (3 user, 3 assistant), image=True, img_size=(666, 797)\n",
      "  sample 2: 6 turns (3 user, 3 assistant), image=True, img_size=(414, 414)\n",
      "  sample 3: 6 turns (3 user, 3 assistant), image=True, img_size=(782, 583)\n",
      "\n",
      "=== IFT collate, batch of 4 multi-turn ===\n",
      "  input_ids            torch.Size([4, 556])           dtype=torch.int64\n",
      "  attention_mask       torch.Size([4, 556])           dtype=torch.int64\n",
      "  token_type_ids       torch.Size([4, 556])           dtype=torch.int64\n",
      "  pixel_values         torch.Size([4, 3, 896, 896])   dtype=torch.float32\n",
      "  labels               torch.Size([4, 556])           dtype=torch.int64\n",
      "\n",
      "  sample 0: non-pad=501  pad=55  trained=198  ratio=39.5%\n",
      "    model turns trained: 3 / 3 expected\n",
      "\n",
      "  sample 1: non-pad=553  pad=3  trained=244  ratio=44.1%\n",
      "    model turns trained: 3 / 3 expected\n",
      "\n",
      "  sample 2: non-pad=556  pad=0  trained=243  ratio=43.7%\n",
      "    model turns trained: 3 / 3 expected\n",
      "\n",
      "  sample 3: non-pad=443  pad=113  trained=139  ratio=31.4%\n",
      "    model turns trained: 3 / 3 expected\n",
      "\n",
      "\n",
      "multi-turn batch: ALL PASSED\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Multi-turn batch test (4 samples, each multi-turn, 1 image)\n",
    "import json\n",
    "\n",
    "# grab 4 multi-turn samples from the actual dataset\n",
    "multi_turn_samples = []\n",
    "for i in range(len(ds)):\n",
    "    image, convo = ds[i]\n",
    "    if len(convo) >= 6:  # at least 3 Q&A pairs\n",
    "        multi_turn_samples.append((image, convo))\n",
    "    if len(multi_turn_samples) == 4:\n",
    "        break\n",
    "\n",
    "print(f\"found {len(multi_turn_samples)} multi-turn samples\")\n",
    "for i, (img, convo) in enumerate(multi_turn_samples):\n",
    "    n_turns = len(convo)\n",
    "    n_user = sum(1 for t in convo if t[\"role\"] == \"user\")\n",
    "    n_asst = sum(1 for t in convo if t[\"role\"] == \"assistant\")\n",
    "    has_image = any(\n",
    "        b.get(\"type\") == \"image\"\n",
    "        for t in convo\n",
    "        for b in (t[\"content\"] if isinstance(t[\"content\"], list) else [])\n",
    "    )\n",
    "    print(f\"  sample {i}: {n_turns} turns ({n_user} user, {n_asst} assistant), image={has_image}, img_size={img.size}\")\n",
    "\n",
    "# run through ift collate\n",
    "batch = dm.ift_train_collate_fn(multi_turn_samples)\n",
    "\n",
    "print(f\"\\n=== IFT collate, batch of {len(multi_turn_samples)} multi-turn ===\")\n",
    "for k, v in batch.items():\n",
    "    if hasattr(v, \"shape\"):\n",
    "        print(f\"  {k:20s} {str(v.shape):30s} dtype={v.dtype}\")\n",
    "\n",
    "# per-sample breakdown\n",
    "for i in range(len(multi_turn_samples)):\n",
    "    input_ids = batch[\"input_ids\"][i]\n",
    "    labels = batch[\"labels\"][i]\n",
    "    \n",
    "    n_total = (input_ids != processor.tokenizer.pad_token_id).sum().item()\n",
    "    n_trained = (labels != -100).sum().item()\n",
    "    n_pad = (input_ids == processor.tokenizer.pad_token_id).sum().item()\n",
    "    \n",
    "    # count how many model turns are being trained\n",
    "    trained_text = processor.tokenizer.decode(\n",
    "        [t for t, l in zip(input_ids.tolist(), labels.tolist()) if l != -100]\n",
    "    )\n",
    "    n_model_turns_trained = trained_text.count(\"<start_of_turn>model\")\n",
    "    n_model_turns_expected = sum(1 for t in multi_turn_samples[i][1] if t[\"role\"] == \"assistant\")\n",
    "    \n",
    "    print(f\"\\n  sample {i}: non-pad={n_total}  pad={n_pad}  trained={n_trained}  ratio={n_trained/max(n_total,1):.1%}\")\n",
    "    print(f\"    model turns trained: {n_model_turns_trained} / {n_model_turns_expected} expected\")\n",
    "    \n",
    "    assert n_model_turns_trained == n_model_turns_expected, \\\n",
    "        f\"sample {i}: trained {n_model_turns_trained} model turns but expected {n_model_turns_expected}\"\n",
    "    assert n_trained > 0, f\"sample {i} has 0 trained tokens\"\n",
    "\n",
    "    # verify no user text leaked\n",
    "    for t in multi_turn_samples[i][1]:\n",
    "        if t[\"role\"] == \"user\":\n",
    "            for block in t[\"content\"]:\n",
    "                if block.get(\"type\") == \"text\":\n",
    "                    # grab first 30 chars of user question as a check\n",
    "                    snippet = block[\"text\"][:30]\n",
    "                    assert snippet not in trained_text, \\\n",
    "                        f\"sample {i}: user text leaked into trained tokens: '{snippet}'\"\n",
    "\n",
    "print(\"\\n\\nmulti-turn batch: ALL PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f00218d-51b1-4b1c-aef2-edf64a2c5dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: pad at positions 501..555 (end=555)\n",
      "sample 1: pad at positions 553..555 (end=555)\n",
      "sample 2: no padding\n",
      "sample 3: pad at positions 443..555 (end=555)\n"
     ]
    }
   ],
   "source": [
    "# quick check: where are the pad tokens?\n",
    "for i in range(4):\n",
    "    ids = batch[\"input_ids\"][i]\n",
    "    pad_positions = (ids == processor.tokenizer.pad_token_id).nonzero(as_tuple=True)[0]\n",
    "    if len(pad_positions) > 0:\n",
    "        print(f\"sample {i}: pad at positions {pad_positions[0].item()}..{pad_positions[-1].item()} (end={ids.shape[0]-1})\")\n",
    "    else:\n",
    "        print(f\"sample {i}: no padding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93097691-74fc-4cd3-924b-cdad4beb1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 883/883 [00:13<00:00, 66.65it/s, Materializing param=model.vision_tower.vision_model.post_layernorm.weight]                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded google/medgemma-4b-it\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Load model and extract activations from the batch\n",
    "from transformers import AutoModelForImageTextToText\n",
    "import gc\n",
    "\n",
    "model_id = \"google/medgemma-4b-it\"  # or \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"loaded {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a432cc79-ea2f-47f1-9398-5466d6c153a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: 35\n",
      "shape per layer: torch.Size([4, 556, 2560])\n",
      "  (batch=4, seq_len=556, hidden_dim=2560)\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Forward pass, extract hidden states\n",
    "# move batch to device\n",
    "device = model.device\n",
    "inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "\n",
    "# cast pixel values to match model dtype\n",
    "inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(dtype=torch.bfloat16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "print(f\"layers: {len(outputs.hidden_states)}\")\n",
    "print(f\"shape per layer: {outputs.hidden_states[0].shape}\")\n",
    "print(f\"  (batch={outputs.hidden_states[0].shape[0]}, \"\n",
    "      f\"seq_len={outputs.hidden_states[0].shape[1]}, \"\n",
    "      f\"hidden_dim={outputs.hidden_states[0].shape[2]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40929775-6a00-44d1-823f-c2d6514a9552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "104dd29b-53bd-4e01-a4bc-3aa895b40901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0: 3 assistant turns, 198 tokens\n",
      "  ranges: [(275, 298), (309, 424), (440, 500)]\n",
      "  text: <start_of_turn>model\n",
      "The main finding in the image is bilateral lower lobe consolidation, which is present in both lungs...\n",
      "  last layer shape: torch.Size([198, 2560])\n",
      "\n",
      "sample 1: 3 assistant turns, 244 tokens\n",
      "  ranges: [(275, 322), (336, 408), (427, 552)]\n",
      "  text: <start_of_turn>model\n",
      "The CT scan is presented in a coronal view. This view is a vertical plane that divides the body int...\n",
      "  last layer shape: torch.Size([244, 2560])\n",
      "\n",
      "sample 2: 3 assistant turns, 243 tokens\n",
      "  ranges: [(279, 328), (341, 452), (472, 555)]\n",
      "  text: <start_of_turn>model\n",
      "The main difference between this image and the previous one is the marked decrease in the size of t...\n",
      "  last layer shape: torch.Size([243, 2560])\n",
      "\n",
      "sample 3: 3 assistant turns, 139 tokens\n",
      "  ranges: [(276, 316), (330, 367), (380, 442)]\n",
      "  text: <start_of_turn>model\n",
      "The image shows a histopathologic photograph of a keloid, which is a type of scar tissue that forms...\n",
      "  last layer shape: torch.Size([139, 2560])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Extract assistant-only activations per sample\n",
    "# for each sample, find where the model (assistant) turns are\n",
    "# using the same boundary logic as the collate\n",
    "\n",
    "start_of_turn_id = processor.tokenizer.convert_tokens_to_ids(\"<start_of_turn>\")\n",
    "end_of_turn_id = processor.tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "model_token_id = processor.tokenizer.convert_tokens_to_ids(\"model\")\n",
    "\n",
    "all_sample_activations = []  # list of dicts per sample\n",
    "\n",
    "for i in range(batch[\"input_ids\"].shape[0]):\n",
    "    ids = batch[\"input_ids\"][i]\n",
    "    \n",
    "    # find assistant token ranges\n",
    "    assistant_ranges = []\n",
    "    in_model_turn = False\n",
    "    turn_start = None\n",
    "    \n",
    "    for j in range(len(ids)):\n",
    "        tok = ids[j].item()\n",
    "        if tok == start_of_turn_id:\n",
    "            if j + 1 < len(ids) and ids[j + 1].item() == model_token_id:\n",
    "                in_model_turn = True\n",
    "                turn_start = j\n",
    "            else:\n",
    "                in_model_turn = False\n",
    "        if tok == end_of_turn_id and in_model_turn:\n",
    "            assistant_ranges.append((turn_start, j + 1))  # inclusive of end_of_turn\n",
    "            in_model_turn = False\n",
    "    \n",
    "    # extract activations for assistant ranges across all layers\n",
    "    sample_hs = {}\n",
    "    for layer_idx, hs in enumerate(outputs.hidden_states):\n",
    "        layer_acts = []\n",
    "        for start, end in assistant_ranges:\n",
    "            layer_acts.append(hs[i, start:end, :].detach().cpu().float())\n",
    "        # concatenate all assistant turns for this layer\n",
    "        if layer_acts:\n",
    "            sample_hs[layer_idx] = torch.cat(layer_acts, dim=0)\n",
    "    \n",
    "    all_sample_activations.append({\n",
    "        \"assistant_ranges\": assistant_ranges,\n",
    "        \"hidden_states\": sample_hs,\n",
    "    })\n",
    "    \n",
    "    # print summary\n",
    "    total_asst_tokens = sum(e - s for s, e in assistant_ranges)\n",
    "    decoded_asst = processor.tokenizer.decode(\n",
    "        [ids[j].item() for s, e in assistant_ranges for j in range(s, e)]\n",
    "    )\n",
    "    print(f\"sample {i}: {len(assistant_ranges)} assistant turns, {total_asst_tokens} tokens\")\n",
    "    print(f\"  ranges: {assistant_ranges}\")\n",
    "    print(f\"  text: {decoded_asst[:120]}...\")\n",
    "    print(f\"  last layer shape: {sample_hs[len(outputs.hidden_states)-1].shape}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "786d20ba-046f-4c3c-bf12-3de109460da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Activation stats (assistant tokens only) ===\n",
      "\n",
      "sample 0 (198 assistant tokens):\n",
      "  layer  0:  mean=0.0071  std=1.0164  min=-28.7500  max=17.5000\n",
      "  layer 17:  mean=12.2498  std=610.5491  min=-1128.0000  max=38912.0000\n",
      "  layer 34:  mean=0.0501  std=3.6441  min=-144.0000  max=104.0000\n",
      "\n",
      "sample 1 (244 assistant tokens):\n",
      "  layer  0:  mean=0.0112  std=1.0165  min=-28.7500  max=17.5000\n",
      "  layer 17:  mean=12.2320  std=616.3966  min=-1152.0000  max=37376.0000\n",
      "  layer 34:  mean=0.0537  std=3.5127  min=-133.0000  max=103.0000\n",
      "\n",
      "sample 2 (243 assistant tokens):\n",
      "  layer  0:  mean=0.0099  std=1.0172  min=-28.7500  max=17.5000\n",
      "  layer 17:  mean=12.4186  std=618.2217  min=-1096.0000  max=40448.0000\n",
      "  layer 34:  mean=0.0690  std=3.5310  min=-107.0000  max=113.5000\n",
      "\n",
      "sample 3 (139 assistant tokens):\n",
      "  layer  0:  mean=0.0069  std=1.0159  min=-28.7500  max=17.5000\n",
      "  layer 17:  mean=12.4201  std=616.0704  min=-1016.0000  max=37376.0000\n",
      "  layer 34:  mean=0.0772  std=4.2014  min=-157.0000  max=128.0000\n",
      "\n",
      "no NaNs or Infs: PASSED\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Sanity check: activation stats\n",
    "n_layers = len(outputs.hidden_states)\n",
    "\n",
    "print(\"=== Activation stats (assistant tokens only) ===\")\n",
    "for sample_idx in range(len(all_sample_activations)):\n",
    "    hs = all_sample_activations[sample_idx][\"hidden_states\"]\n",
    "    first = hs[0]\n",
    "    mid = hs[n_layers // 2]\n",
    "    last = hs[n_layers - 1]\n",
    "    \n",
    "    print(f\"\\nsample {sample_idx} ({first.shape[0]} assistant tokens):\")\n",
    "    print(f\"  layer  0:  mean={first.mean():.4f}  std={first.std():.4f}  min={first.min():.4f}  max={first.max():.4f}\")\n",
    "    print(f\"  layer {n_layers//2:2d}:  mean={mid.mean():.4f}  std={mid.std():.4f}  min={mid.min():.4f}  max={mid.max():.4f}\")\n",
    "    print(f\"  layer {n_layers-1:2d}:  mean={last.mean():.4f}  std={last.std():.4f}  min={last.min():.4f}  max={last.max():.4f}\")\n",
    "\n",
    "# verify no NaNs or Infs\n",
    "for sample_idx in range(len(all_sample_activations)):\n",
    "    for layer_idx, acts in all_sample_activations[sample_idx][\"hidden_states\"].items():\n",
    "        assert not torch.isnan(acts).any(), f\"NaN in sample {sample_idx} layer {layer_idx}\"\n",
    "        assert not torch.isinf(acts).any(), f\"Inf in sample {sample_idx} layer {layer_idx}\"\n",
    "\n",
    "print(\"\\nno NaNs or Infs: PASSED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4b5998e-1ff7-41eb-9ae8-124cfac7312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mem after cleanup: 8.6 GB\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Cleanup\n",
    "del outputs, inputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU mem after cleanup: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "801ab492-5d52-4884-962a-936b66572b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision layers: 28\n",
      "shape per layer: torch.Size([4, 4096, 1152])\n",
      "  (batch=4, patches=4096, vision_dim=1152)\n",
      "  first   : mean=0.3025  std=1.9446\n",
      "  mid (14): mean=0.0445  std=15.1405\n",
      "  last    : mean=-1.3789  std=99.9512\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Extract vision encoder hidden states\n",
    "\n",
    "# option 1: pass pixel_values directly through the vision tower\n",
    "pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.bfloat16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.model.vision_tower(\n",
    "        pixel_values, \n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "print(f\"vision layers: {len(vision_outputs.hidden_states)}\")\n",
    "print(f\"shape per layer: {vision_outputs.hidden_states[0].shape}\")\n",
    "print(f\"  (batch={vision_outputs.hidden_states[0].shape[0]}, \"\n",
    "      f\"patches={vision_outputs.hidden_states[0].shape[1]}, \"\n",
    "      f\"vision_dim={vision_outputs.hidden_states[0].shape[2]})\")\n",
    "\n",
    "# stats\n",
    "for idx in [0, len(vision_outputs.hidden_states)//2, -1]:\n",
    "    hs = vision_outputs.hidden_states[idx].float()\n",
    "    label = {0: \"first\", -1: \"last\"}.get(idx, f\"mid ({idx})\")\n",
    "    print(f\"  {label:8s}: mean={hs.mean():.4f}  std={hs.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbba7a62-40d1-4511-a546-eb1cf06c7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 — Vision encoder output: torch.Size([4, 4096, 1152])\n",
      "  (batch, patches=4096, vision_dim=1152)\n",
      "Stage 2 — After projector:       torch.Size([4, 256, 2560])\n",
      "  (batch, compressed=256, llm_dim=2560)\n",
      "Stage 3 — Image positions in LLM: 256 tokens at positions 15..270\n",
      "  LLM last layer at image positions: torch.Size([256, 2560])\n",
      "\n",
      "=== Dimension summary ===\n",
      "  vision encoder:  4096 patches × 1152d\n",
      "  after projector: 256 tokens × 2560d\n",
      "  LLM image slots: 256 tokens × 2560d\n"
     ]
    }
   ],
   "source": [
    "# %% Cell — Vision activations: before and after projection\n",
    "\n",
    "pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.bfloat16)\n",
    "\n",
    "# Stage 1: raw vision encoder (you already have this)\n",
    "with torch.no_grad():\n",
    "    vision_outputs = model.model.vision_tower(\n",
    "        pixel_values,\n",
    "        output_hidden_states=True,\n",
    "    )\n",
    "\n",
    "vision_last = vision_outputs.hidden_states[-1]  # or .last_hidden_state\n",
    "print(f\"Stage 1 — Vision encoder output: {vision_last.shape}\")\n",
    "print(f\"  (batch, patches={vision_last.shape[1]}, vision_dim={vision_last.shape[2]})\")\n",
    "\n",
    "# Stage 2: after multimodal projector (4096 patches -> 256 tokens, 1152 -> 2560)\n",
    "with torch.no_grad():\n",
    "    projected = model.model.multi_modal_projector(vision_last)\n",
    "\n",
    "print(f\"Stage 2 — After projector:       {projected.shape}\")\n",
    "print(f\"  (batch, compressed={projected.shape[1]}, llm_dim={projected.shape[2]})\")\n",
    "\n",
    "# Stage 3: find the image token positions in the LLM hidden states\n",
    "# these are the same 256 positions, but after going through LLM attention layers\n",
    "image_token_id = processor.image_token_id\n",
    "sample_ids = batch[\"input_ids\"][0]\n",
    "image_positions = (sample_ids == image_token_id).nonzero(as_tuple=True)[0]\n",
    "print(f\"Stage 3 — Image positions in LLM: {len(image_positions)} tokens at positions {image_positions[0].item()}..{image_positions[-1].item()}\")\n",
    "\n",
    "# extract LLM hidden states at image positions for each layer\n",
    "# (you already have outputs.hidden_states from the earlier cell)\n",
    "llm_image_hs_last = outputs.hidden_states[-1][0, image_positions, :].detach().cpu().float()\n",
    "print(f\"  LLM last layer at image positions: {llm_image_hs_last.shape}\")\n",
    "\n",
    "# compare all three\n",
    "print(f\"\\n=== Dimension summary ===\")\n",
    "print(f\"  vision encoder:  {vision_last.shape[1]} patches × {vision_last.shape[2]}d\")\n",
    "print(f\"  after projector: {projected.shape[1]} tokens × {projected.shape[2]}d\")\n",
    "print(f\"  LLM image slots: {len(image_positions)} tokens × {outputs.hidden_states[-1].shape[2]}d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47540c32-325e-45a1-9b2f-be143ace75bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
